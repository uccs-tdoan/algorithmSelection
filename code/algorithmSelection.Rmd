---
title: "Regression Models"
output: pdf_document
author: Tri Doan
Date: March 3, 2015 Last update: June 25, 2015
---

The following codes demonstrate algorithm selection  project.

```{r,echo=FALSE}
library(caret)
library(e1071)
library(ggplot2)
library(MASS)
library(gbm)

setwd("C:/algorithmSelection/code")
```

Since dataset contains all features that might be not all useful. We eliminate first 6 attributes as well as 25th feature (Dataset feature).



```{r, echo=FALSE}
df <- read.csv("../data/Alldata.csv")

df <- df[-c(1:7,10,12:14)]
```
Next, we convert category feature (algorithm attribute) into binary feature.
```{r}
simpleMod <- dummyVars(~., data=df,levelsOnly= TRUE)
df <- predict(simpleMod, df)
df <- as.data.frame(df)

write.table(df,file="Train.csv",sep=",",row.names=FALSE,col.names=TRUE)    
```
 We can use Trains.csv or continue for remaining code
 
```{r, echo=FALSE}
 set.seed(215)
 TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= FALSE)

 trainData <- df[TrainRow,]
 ctrl <- trainControl(method = "repeatedcv", repeats = 5,number=10)

 trainX <- df[TrainRow,1:ncol(df)-1]
 trainY<-df[TrainRow,ncol(df)]
 testX <- df[-TrainRow, 1:ncol(df)-1]
 observed <- df[-TrainRow,ncol(df)]


```
We use different regression techniques such as
Non-linear regression with decision Tree includes 
1. Classification and Regression Trees
2. Conditional Decision Trees
3. Model Trees
4. Rule System
5. Bagging CART
6. Random Forest
7. Gradient Boosted Machine
8. Cubist

Other non-linear regression includes
1. Neuron Network
2. Principle Components Regression
3. generate Support Vector Regression (SVR)
4. generate KNN regression
5. Partial Least Square (PLS)
6. Generate Elastic Net model
7. generate Ridge Regression
8. generate LARS model
9. Generate Multivariate Adaptive Regression Splines  (MARS)

loading libraries
```{R }
library(rpart)
library(party)
library(RWeka)
library(ipred)
library(randomForest)
library(gbm)
library(Cubist)

```
1. Classification and Regression Trees
  rmse = 0.9290579
  Spearman's rank correlation rho = 0.7721002
  S = 17408251, p-value < 2.2e-16
  
```{r}
fit <- rpart(SAR~., data=trainData, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predCART <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCART)^2)
result <- cbind(observed,predCART)
CART_residual <- predCART - observed

cor.test(observed,predCART, method="spearman")
```
 2. Conditional Decision Trees
 RMSE = 0.9165631
 
 Spearman's rank correlation rho = 0.8856459 
 S = 8735001, p-value < 2.2e-16
 
```{r}
# fit model
fit <- ctree(SAR~., data=trainData, controls=ctree_control(minsplit=2,minbucket=2,testtype="Univariate"))
# summarize the fit
summary(fit)
# make predictions
predCTree <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCTree)^2)
result <- cbind(result,predCTree)
CTree_residual <- predCTree - observed

cor.test(observed,predCTree, method="spearman")

```
3. Model Tree (M5P in weka)

We tune M5P and using MP5 tree to calculate
```{r}
 m5Tune <- train(trainX, trainY, 
                 method = "M5", 
                 trControl = trainControl(method = "cv"), 
                 control = Weka_control(M = 10))

plot(m5Tune)
m5tree <- M5P(trainY ~ ., data = trainX,  control = Weka_control(M = 10))

# Alternative, directly us  RWeka package
# 
# RMSE = 0.9308284
# Spearman's rank correlation rho = 0.6438348 
# S = 27205874, p-value < 2.2e-16


fit <- M5P(SAR~., data=trainData)
# summarize the fit
summary(fit)
# make predictions
predM5P <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predM5P)^2)
result <- cbind(result,predM5P)
M5P_residual <- predM5P - observed

cor.test(observed,predM5P, method="spearman")

```
4. Rule based System
RMSE= 0.9165631

Spearman's rank correlation rho = 0.6438348 
S = 27205874, p-value < 2.2e-16

```{r}

# fit model
fit <- M5Rules(SAR~., data= trainX)
# summarize the fit
summary(fit)
# make predictions
predRule <- predict(fit, testX)
# summarize accuracy
rmse <- mean(( observed - predRule)^2)
result <- cbind(result,predRule)
Rule_residual <- predRule - observed

cor.test(observed, predRule, method="spearman")
```
5. Bagging CART
   RMSE = 0.9251395
   
   Spearman's rank correlation rho = 0.8373852
   
   S = 12421416, p-value < 2.2e-16
  
```{r}  

# fit model
fit <- bagging(SAR~., data= trainData, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predBagCART <- predict(fit, testX)
# summarize accuracy
rmse <- mean(( observed- predBagCART)^2)
result <- cbind(result,predBagCART)
BagCart_residual <- predBagCART - observed

cor.test(observed,predBagCART,  method="spearman")

```
6. Random Forest
   RMSE = 0.9215616
   Spearman's rank correlation rho = 0.8976872 
   S = 7815221, p-value < 2.2e-16

```{r}
# fit model
fit <- randomForest(SAR~., data=trainData)
# summarize the fit
summary(fit)
# make predictions
predRF <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predRF)^2)

cor.test(observed,predRF,  method="spearman")

```
7. Gradient Boosted Regression
   RMSE = 0.9439175
   
   Spearman's rank correlation rho = 0.3470242 
   S = 49877913, p-value < 2.2e-16
   
```{r}   
# fit model
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predGBoost <- predict(fit,df[-TrainRow, ],n.trees=5)
# summarize accuracy
rmse <- mean(( observed - predGBoost)^2)
result <- cbind(result, predGBoost)
GBoost_residual <- predGBoost - observed

cor.test(observed, predGBoost, method="spearman")

```
8. Cubist
 RMSE = 0.902461
 Spearman's rank correlation rho = 0.8806595 
 S = 9115892, p-value < 2.2e-16
 

```{r}
# fit model
fit <- cubist(trainX, trainY)
# summarize the fit
summary(fit)
# make predictions
predCubist <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCubist)^2)


cor.test(observed, predCubist, method="spearman")

```

Non-linear regression 
1. Neuron Network 

Note: Compute confidence interval of prediction
predict(model, new = testX, inteval= "prediction")
predict(model, new = testX, inteval= "confidence")
build neuron network with 5 hidden unit
RMSE =  0.9699801

Spearman's rank correlation rho = 0.8103927
S = 14483256, p-value < 2.2e-16

```{r}
set.seed(215)
nnetAvg <- avNNet(trainX, trainY,  size = 5,  decay = 0.01,  repeats = 5,
        linout = TRUE, trace = FALSE,  maxit = 500,   allowParallel= TRUE)
predNN <- predict(nnetAvg, testX)
RMSE(predNN,observed)
result <- cbind(observed,predNN)
NN_residual <- predNN - observed

cor.test(observed, predNN, method="spearman")
rm(predNN)
```
0.  Built Principle Components Regression model
 RMSE=  0.9713516
 Spearman's rank correlation rho  = 0.09964621
 S = 68774010, p-value = 0.005618
 
```{r,echo=FALSE}
set.seed(215)
runPCR <- train(x = trainX, y = trainY, method = "pcr", trControl = ctrl,tuneLenght=25)
predPCR <- predict(runPCR, newdata = testX)

RMSE(predPCR,observed)

result <- cbind(result,predPCR)
pcr_residual = predPCR - observed
cor.test(observed, predPCR, method="spearman")
rm(predPCR)


```
2. generate Support Vector Regression (SVR) model
  RMSE=   0.9640276
  Spearman's rank correlation rho = 0.724632 
  S = 21034137, p-value < 2.2e-16
  
```{r, echo=FALSE}
set.seed(215)  
library(kernlab)
svmRModel <- train(x = trainX,trainY,method = "svmRadial",preProc = c("center", "scale"), tuneLength = 10,trControl = trainControl(method = "cv")) 
#Tuning parameter 'sigma' was held constant at a value of 0.0108882
#RMSE was used to select the optimal model using  the smallest value.
#The final values used for the model were sigma = 0.0108882 and C = 16.
svmRModel$finalModel
#SV type: eps-svr  (regression) 
#parameter : epsilon = 0.1  cost C = 8
#svmFit <- ksvm(x = trainX, y = trainY,kernel ="rbfdot", kpar = "automatic",C = 8, epsilon = 0.1)


predSVMR <- predict(svmRModel, newdata = testX)
RMSE(predSVMR,observed)  # 1807.155 after Box Cox, RMSE : 1.393408 
result <- cbind(result,predSVMR)
SVMR_residual <- predSVMR - observed
residuals <- cbind(residuals, SVMR_residual)


cor.test(observed, predSVMR, method="spearman")
rm(SVMR_residual)
rm(predSVMR)

```
3.  generate KNN regression
 RMSE = 0.9691691
 Spearman's rank correlation rho = 0.4849237 
 
 S = 39344378, p-value < 2.2e-16
 
```{r , echo = FALSE}
set.seed(215)
knnTuned <- train(trainX, trainY,method = "knn",preProc = c("center", "scale"),
                  tuneGrid = data.frame(.k = 1:20),trControl = trainControl(method = "cv"))
predKNN <- predict(knnTuned,testX)
RMSE(predKNN, observed)
result <- cbind(result,predKNN)

KNNR_residual <- predKNN - observed
residuals <- cbind(residuals, KNNR_residual)


cor.test(observed, predKNN, method="spearman")
rm(KNNR_residual)
rm(predKNN)

```
4. Generate Partial Least Square (PLS) model
 RMSE = 0.9669828
 Spearman's rank correlation rho = 0.5618751 
 S = 33466406, p-value < 2.2e-16

```{r,echo=FALSE}
set.seed(215)
runPLS <- train(x = trainX, y = trainY, method = "pls", preProcess=c("center","scale"),trControl = ctrl,tuneLenght=25)
predPLS <- predict(runPLS, newdata = testX)

RMSE(predPLS,observed)
result <- cbind(result,predPLS)
cor.test(observed, predPLS, method="spearman")

PLS_residual <- predPLS - observed
residuals <- cbind(pcr_residual,PLS_residual)
rm(predPLS)
rm(PLS_residual)


```
 5. generate Ridge Regression
 RMSE = 0.9669102
 Spearman's rank correlation = 0.5731067
 S = 32608475, p-value < 2.2e-16
 
```{r, echo=FALSE}
set.seed(215)
ridgeGrid <- data.frame(.lambda = seq( .1,4, length = 20))
ridgeRegFit <- train(trainX,trainY, method ="ridge",tuneGrid = ridgeGrid, trControl=ctrl, preProc=c("center","scale"))
ridgeRegFit

ridgeModel <- enet(x = as.matrix(trainX), y = trainY,lambda=0.1)
ridgePred <- predict(ridgeModel,newx=as.matrix(testX),s=1,mode="fraction",type="fit")


RMSE(ridgePred$fit,observed)
predRidge <- ridgePred$fit 
result <- cbind(result,predRidge)

cor.test(observed, predRidge, method="spearman")

ridge_residual <- predRidge - observed
residuals <- cbind(residuals,ridge_residual)
rm(ridge_residual)


rm(predRidge)
rm(ridgePred)  

```
 6. generate LARS model
 RMSE=   0.9668436
 Spearman's rank correlation = 0.5790173 
 S = 32156992, p-value < 2.2e-16
 
 eg: lassoFit <- lars(as.matrix(trainX),trainY,type=c("lasso"))
 lassoPred <- predict.lars(lassoFit,newx=as.matrix(testX),type="fit")

 RMSE(lassoPred$fit,observed)
 predLasso <- lassoPred$fit 
  
```{r, echo=FALSE}
set.seed(215)
library(lars)
# compute MSEs for a range of coefficient penalties as a fraction of 
# final L1 norm on the interval [0,1] using cross validation
cv.res <- cv.lars(as.matrix(trainX),trainY,type="lasso",mode="fraction",plot=FALSE)
# Choose optimal value one standarf deviation away from minimum MSE
opt.frac <- min(cv.res$cv) + sd(cv.res$cv)
opt.frac <-cv.res$index[which(cv.res$cv < opt.frac)[1]]
# Compute LARS fit
lars.fit <- lars(as.matrix(trainX),trainY,type=c("lasso"))
larsPred <- predict.lars(lars.fit,newx=as.matrix(testX),type="fit")
# get last model fit
last.fit <- dim(larsPred[4]$fit)[2]

predLars <- larsPred[4]$fit[,last.fit]

RMSE(predLars,observed)

cor.test(observed, predLars, method="spearman")
result <- cbind(result,predLars)

lars_residual <- predLars - observed
residuals <- cbind(residuals, lars_residual)
rm(lars_residual)


rm(predLars)
rm(larsPred)  

```
7. Generate Elastic Net model
 RMSE  =  0.9668436    
 Spearman's rank correlation = 0.5790173 
 S = 32156992, p-value < 2.2e-16
 

```{r, echo=FALSE}
set.seed(215)  
library(elasticnet)
enetGrid <- expand.grid(lambda = c(0, .001, .01, .1, 1), fraction = seq(.05, 1, length = 20))
runENet <-  train(x = trainX, y = trainY, method = "enet", preProcess=c("center","scale"),trControl = ctrl,
                  tuneGrid=enetGrid) 
enetModel <- enet(x = as.matrix(trainX), y = trainY, lambda = 0, normalize = TRUE)
predEnet <- predict(enetModel, newx = as.matrix(testX), s = .1, mode = "fraction",    type = "fit")

RMSE(predEnet$fit,observed)
Elastic <- predEnet$fit

result <- cbind(result,Elastic)

Elastic_residual <- predEnet$fit - observed
residuals <- cbind(residuals,Elastic_residual)
rm(Elastic_residual)

cor.test(observed, Elastic, method="spearman")
rm(predEnet)  
rm(Elastic)
```
 8. Generate MARS
 RMSE =  0.9626668
 Spearman's rank correlation = 0.8385391
 S = 12333280, p-value < 2.2e-16

```{r, echo= FALSE}
set.seed(215)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(trainX, trainY, method = "earth",tuneGrid = marsGrid,trControl = trainControl(method = "cv"))
predMars <- predict(marsTuned,testX)
RMSE(predMars,observed)
result <- cbind(result,predMars)

cor.test(observed, predMars, method="spearman")
mars_residual <- predMars - observed
residuals <- cbind(residuals, mars_residual)

rm(mars_residual)
rm(predMars) 

# alternatively, postResample(pred = knnPred, obs = testY)
write.table(result,file="preditedSAR.csv",sep=",",row.names=FALSE,col.names=TRUE)    
write.table(residuals,file="residuals_Compare.csv",sep=",",row.names=FALSE,col.names=TRUE)    

```
  plot residual histogram of regression models

```{r, echo=FALSE}
 df <- read.csv("residuals_compare.csv")
 dat <- melt(df)
 pdf("residualHistogram.pdf", width=6, height=5)
 p1 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'pcr_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("PCR residuals")

 p2 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'PLS_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("PLS residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p3 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'Elastic_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("Elastic Net residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p4 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'ridge_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("ridge residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p5 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'lars_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("Lars residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p6 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'SVMR_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("SVR residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))


p7 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'mars_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("mars residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

p8 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'KNNR_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("KNN residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

p<- grid.arrange(p1,p2, p3, p4,p5,p6,p7,p8, nrow=2)
print(p)
 
dev.off()
```
 compute 95% confidence interval (CI)
```{r, echo=FALSE}
 # PCR : 2.5% = -5.735264 ; 97.5% =  9.441427 
 quantile(df$pcr_residual,.025) 
 quantile(df$pcr_residual,.975)
 
 # PLS:  2.5% = -10.52431;  97.5% = 5.875617 
 quantile(df$PLS_residual,.025) 
 quantile(df$PLS_residual,.975)
 
 # Elastic: 2.5% = -7.38707  ; 97.5% = 5.639585 
 quantile(df$Elastic_residual,.025) 
 quantile(df$Elastic_residual,.975)

 # Ridge: 2.5% = -10.46934  ; 97.5% = 5.877543 
 quantile(df$ridge_residual,.025) 
 quantile(df$ridge_residual,.975)

 # LARS: 2.5% = -5.214255 ; 97.5% = 5.171606  
 quantile(df$lars_residual,.025) 
 quantile(df$lars_residual,.975)

 # SVMR_residual : 2.5% = -4.025373 ; 97.5% = 3.676516 ; 
 quantile(df$SVMR_residual,.025) 
 quantile(df$SVMR_residual,.975)

 # mars : 2.5% = -1.281899 ; 97.5%  = 1.152502 
 quantile(df$mars_residual,.025) 
 quantile(df$mars_residual,.975)

 # KNNR: 2.5% = -6.479854 ; 97.5% = 9.357834 
 quantile(df$KNNR_residual,.025) 
 quantile(df$KNNR_residual,.975)
```
 compute MAD : Median absolute quitdeviation
```{r, echo= FALSE}
 # MAD_PCR = 1.552573
  mad_pcr = mad(df$pcr_residual,center=median(df$pcr_residual))
 
 # MAD_PLS = 2.425057
  mad_pls = mad(df$PLS_residual,center=median(df$PLS_residual))
 
 # MAD_Elastic = 2.608583
  mad_enet = mad(df$Elastic_residual,center=median(df$Elastic_residual))

 # MAD_Ridge = 2.185359
 mad_ridge = mad(df$ridge_residual,center=median(df$ridge_residual))
 
 # MAD_Lars = 1.949095
 mad_lars = mad(df$lars_residual,center=median(df$lars_residual))

 # MAD_SVR = 1.376014
 mad_svr = mad(df$SVMR_residual,center=median(df$SVMR_residual))

 # MAD_MARS =  0.3343891
  mad_mars  = mad(df$mars_residual,center=median(df$mars_residual))

 # MAD_KNN = 1.489425
  mad_knn = mad(df$KNNR_residual,center=median(df$KNNR_residual))
 
```
 compute Mean Absolute Error
```{r,echo=FALSE}
 # MAE_PCR =  2.807746
  mae_pcr = mean(abs(df$pcr_residual))
 
 # MAE_PLS = 2.379797
  mae_pls = mean(abs(df$PLS_residual))
 
 # MAE_Elastic = 2.098058
  mae_enet = mean(abs(df$Elastic_residual))

 # MAE_Ridge = 2.229552
 mae_ridge = mean(abs(df$ridge_residual))
 
 # MAE_Lars = 1.816003
 mae_lars = mean(abs(df$lars_residual))

 # MAE_SVR = 1.265414
 mae_svr = mean(abs(df$SVMR_residual))

 # MAE_MARS = 0.4152009
  mae_mars  = mean(abs(df$mars_residual))

 # MAE_KNN = 2.563048
  mae_knn = mean(abs(df$KNNR_residual))

```
Compute spearman ranking

PredPCR =  0.5416336
PredPLS =  0.516655
predElastic = 0.5853912
predRidge = 0.5537625
predSVMR = 0.7229657
predMars = 0.834728
predKNN =  0.3462306
```{r}
df <- read.csv("predictedSAR.csv")
with(df,cor(observed,predPCR,method="spearman"))
with(df,cor(observed,predPLS,method="spearman"))
with(df,cor(observed,Elastic,method="spearman"))
     
with(df,cor(observed,predRidge,method="spearman"))
with(df,cor(observed,predSVMR,method="spearman"))

with(df,cor(observed,predMars,method="spearman"))
with(df,cor(observed,predKNN,method="spearman"))
