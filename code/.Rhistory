order(imps)
Library(caret)
library(caret)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(randomForest)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
imps
library(randomForest)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit) # as.factor(cyl)8  -6.0709
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr # 0.1037502
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5] # 0.9945734
hatvalues(fit5)
dfbetas(fit5)[5, 2] # -133.8226
library(dplyr)
install.packages("hflights")
glimpse(hflights)
library(hflights)
glimpse(hflights)
quit()
library(MASS)
quit()
quit()
load(diamond)
load(diamonds)
library(ggthemes)
install.packages('knitr', dependencies = T)
library(knitr)
library(devtools)
install_version("colorspace","1.2-4")
library(ggplot2)
load(diamond)
load(diamonds)
head(diamonds)
dim(diamonds)
str(diamonds)
diamonds$color
?diamond
?diamonds
ggplot(data=diamonds,aes(y=price)) +geom_hist()
ggplot(data=diamonds,aes(y=price)) +geom_histogram()
ggplot(data=diamonds,aes(y=price)) +geom_histogram(stat="bin")
names(diamonds)
qplot(x=price,data=diamonds)
library(ggthemes)
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete(breaks =1:31)
ggplot(aes(x=price),data=diamonds) + geom_histogram()
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete()
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete(breaks =c(0,5000.10000,15000,20000)
)
summary(diamonds)
summary(diamond[diamonds$price <500])
summary(diamonds[diamonds$price <500])
diamonds[diamonds$price <500]
diamonds[diamonds$price <500,]
summary(diamonds[diamonds$price <500,])
dim(diamonds[diamonds$price <500,])
count(diamonds[diamonds$price <500,])
size(diamonds[diamonds$price <500,])
dim(diamonds[diamonds$price <250,])
load(diamonds)
data(diamonds)
dim(diamonds[diamonds$price <250,])
dim(diamonds[diamonds$price > 15000,])
dim(diamonds[diamonds$price >= 15000,])
library("lubridate")
quit()
install.packages("RandomFields")
install.packages(c("arm", "BH", "boot", "caret", "chron", "clue", "cluster", "CORElearn", "devtools", "distrom", "dplyr", "earth", "ergm", "ergm.count", "expsmooth", "FactoMineR", "ForeCA", "forecast", "formatR", "Formula", "gam", "gamlr", "gdata", "geosphere", "ggplot2", "ggvis", "glmnet", "gnm", "gplots", "gtools", "h2o", "HH", "highr", "Hmisc", "jsonlite", "latentnet", "lmtest", "manipulate", "markdown", "MASS", "mda", "mgcv", "mime", "mixtools", "network", "NLP", "numDeriv", "openNLP", "party", "partykit", "pgmm", "plotmo", "plotrix", "plyr", "pROC", "qdap", "qdapDictionaries", "qdapRegex", "Quandl", "R.utils", "Rcpp", "RcppArmadillo", "RCurl", "rgl", "rmarkdown", "robustbase", "ROCR", "roxygen2", "RSNNS", "rstudioapi", "sandwich", "scales", "shiny", "sp", "statmod", "stringr", "strucchange", "survival", "tergm", "testthat", "textcat", "textir", "tm", "vcd", "VGAM", "XML", "yhatr", "zoo"))
install.packages(c("boot", "cluster", "codetools", "lattice", "MASS", "Matrix", "mgcv", "survival"), lib="C:/Program Files/R/R-3.1.3/library")
setwd("C:/algorithmSelection/Data")
library(ipred)
data(BostonHousing)
library(MASS)
data(BostonHousing)
data(BostonHousing,package="MASS")
data(Boston,package="MASS")
mod <- bagging(medv ~ ., data=Boston, coob=TRUE)
print(mod)
learn <- as.data.frame(mlbench.friedman1(200))
library(mlbench)
learn <- as.data.frame(mlbench.friedman1(200))
library(datasets)
# load the package
library(rpart)
# load data
data(longley)
# fit model
fit <- rpart(Employed~., data=longley, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
str(longley)
# load the package
library(party)
# load data
data(longley)
# fit model
fit <- ctree(Employed~., data=longley, controls=ctree_control(minsplit=2,minbucket=2,testtype="Univariate"))
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
predictions
dim(loneley)
dim(longley)
# load the package
library(RWeka)
# load data
data(longley)
# fit model
fit <- M5P(Employed~., data=longley)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
# load the package
library(RWeka)
# load data
data(longley)
# fit model
fit <- M5Rules(Employed~., data=longley)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
# load the package
library(ipred)
# load data
data(longley)
# fit model
fit <- bagging(Employed~., data=longley, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
# load the package
library(randomForest)
# load data
data(longley)
# fit model
fit <- randomForest(Employed~., data=longley)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
# load the package
library(gbm)
# load data
data(longley)
# fit model
fit <- gbm(Employed~., data=longley, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
# load the package
library(Cubist)
# load data
data(longley)
# fit model
fit <- cubist(longley[,1:6], longley[,7])
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley[,1:6])
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
library(caret)
library(e1071)
library(ggplot2)
library(MASS)
setwd("C:/algorithmSelection/code")
df <- read.csv("Alldata.csv")
df <- df[-c(1:7,10,12:14)]
str(df)
df <- read.csv("Alldata.csv")
df <- read.csv("../data/Alldata.csv")
str(df)
dim(df)
df <- df[-c(1:7,10,12:14)]
simpleMod <- dummyVars(~., data=df,levelsOnly= TRUE)
df <- predict(simpleMod, df)
df <- as.data.frame(df)
write.table(df,file="Train.csv",sep=",",row.names=FALSE,col.names=TRUE)
set.seed(215)
TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= FALSE)
trainData <- df[TrainRow,]
ctrl <- trainControl(method = "repeatedcv", repeats = 5,number=10)
trainX <- df[TrainRow,1:ncol(df)-1]
trainY<-df[TrainRow,ncol(df)]
testX <- df[-TrainRow, 1:ncol(df)-1]
observed <- df[-TrainRow,ncol(df)]
str(df)
trainY
str(trainY)
trainX$SAR
trainX$SAR
str(trainX)
str(TrainData)
str(trainRow)
str(TrainRow)
str(trainData)
longley[,1:6]
str(testX)
fit <- rpart(SAR~., data=trainData, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predictions)^2)
rmse
fit <- M5P(SAR~., data=trainData)
# summarize the fit
summary(fit)
# make predictions
predM5P <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predM5P)^2)
rmse
fit <- ctree(Employed~., data=longley, controls=ctree_control(minsplit=2,minbucket=2,testtype="Univariate"))
# summarize the fit
summary(fit)
# make predictions
predCTree <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCTree)^2)
result <- cbind(result,predCTree)
CTree_residual <- predCTree - observed
fit <- ctree(SAR~., data=trainData, controls=ctree_control(minsplit=2,minbucket=2,testtype="Univariate"))
# summarize the fit
summary(fit)
# make predictions
predCTree <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCTree)^2)
result <- cbind(result,predCTree)
CTree_residual <- predCTree - observed
rmse
fit <- M5Rules(SAR~., data= trainX)
# summarize the fit
summary(fit)
# make predictions
predRule <- predict(fit, testX)
# summarize accuracy
rmse <- mean(( observed - predRule)^2)
result <- cbind(result,predRule)
Rule_residual <- predRule - observed
rmse
fit <- bagging(SAR~., data= trainData, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predBagCART <- predict(fit, testX)
# summarize accuracy
rmse <- mean(( observed- predBagCART)^2)
rmse
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predGBoost <- predict(fit,testX )
# summarize accuracy
rmse <- mean(( observed - predGBoost)^2)
str(testX)
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predGBoost <- predict(fit,testX )
data(longley)
# fit model
fit <- gbm(Employed~., data=longley, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
library(gbm)
# load data
data(longley)
# fit model
fit <- gbm(Employed~., data=longley, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
fit <- randomForest(SAR~., data=trainData)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predictions)^2)
rmse
fit <- cubist(trainX, trainY)
# summarize the fit
summary(fit)
# make predictions
predCubist <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCubist)^2)
rmse
set.seed(215)
nnetAvg <- avNNet(trainX, trainY,  size = 5,  decay = 0.01,  repeats = 5,
linout = TRUE, trace = FALSE,  maxit = 500,   allowParallel= TRUE)
predNN <- predict(nnetAvg, testX)
RMSE(predNN,observed)
result <- cbind(observed,predNN)
NN_residual <- predNN - observed
set.seed(215)
library(kernlab)
svmRModel <- train(x = trainX,trainY,method = "svmRadial",preProc = c("center", "scale"), tuneLength = 10,trControl = trainControl(method = "cv"))
#Tuning parameter 'sigma' was held constant at a value of 0.0108882
#RMSE was used to select the optimal model using  the smallest value.
#The final values used for the model were sigma = 0.0108882 and C = 16.
svmRModel$finalModel
#SV type: eps-svr  (regression)
#parameter : epsilon = 0.1  cost C = 8
#svmFit <- ksvm(x = trainX, y = trainY,kernel ="rbfdot", kpar = "automatic",C = 8, epsilon = 0.1)
predSVMR <- predict(svmRModel, newdata = testX)
RMSE(predSVMR,observed)  # 1807.155 after Box Cox, RMSE : 1.393408
result <- cbind(result,predSVMR)
SVMR_residual <- predSVMR - observed
residuals <- cbind(residuals, SVMR_residual)
rm(SVMR_residual)
rm(predSVMR)
set.seed(215)
knnTuned <- train(trainX, trainY,method = "knn",preProc = c("center", "scale"),
tuneGrid = data.frame(.k = 1:20),trControl = trainControl(method = "cv"))
predKNN <- predict(knnTuned,testX)
RMSE(predKNN, observed)
result <- cbind(result,predKNN)
KNNR_residual <- predKNN - observed
residuals <- cbind(residuals, KNNR_residual)
rm(KNNR_residual)
rm(predKNN)
set.seed(215)
runPLS <- train(x = trainX, y = trainY, method = "pls", preProcess=c("center","scale"),trControl = ctrl,tuneLenght=25)
predPLS <- predict(runPLS, newdata = testX)
RMSE(predPLS,observed)
result <- cbind(result,predPLS)
PLS_residual <- predPLS - observed
residuals <- cbind(pcr_residual,PLS_residual)
rm(predPLS)
rm(pcr_residual)
rm(PLS_residual)
set.seed(215)
library(elasticnet)
enetGrid <- expand.grid(lambda = c(0, .001, .01, .1, 1), fraction = seq(.05, 1, length = 20))
runENet <-  train(x = trainX, y = trainY, method = "enet", preProcess=c("center","scale"),trControl = ctrl,
tuneGrid=enetGrid)
enetModel <- enet(x = as.matrix(trainX), y = trainY, lambda = 0, normalize = TRUE)
predEnet <- predict(enetModel, newx = as.matrix(testX), s = .1, mode = "fraction",    type = "fit")
RMSE(predEnet$fit,observed)
Elastic <- predEnet$fit
result <- cbind(result,Elastic)
Elastic_residual <- predEnet$fit - observed
residuals <- cbind(residuals,Elastic_residual)
rm(Elastic_residual)
rm(predEnet)
rm(Elastic)
set.seed(215)
ridgeGrid <- data.frame(.lambda = seq( .1,4, length = 20))
ridgeRegFit <- train(trainX,trainY, method ="ridge",tuneGrid = ridgeGrid, trControl=ctrl, preProc=c("center","scale"))
ridgeRegFit
ridgeModel <- enet(x = as.matrix(trainX), y = trainY,lambda=0.1)
ridgePred <- predict(ridgeModel,newx=as.matrix(testX),s=1,mode="fraction",type="fit")
RMSE(ridgePred$fit,observed)
predRidge <- ridgePred$fit
result <- cbind(result,predRidge)
ridge_residual <- predRidge - observed
residuals <- cbind(residuals,ridge_residual)
rm(ridge_residual)
rm(predRidge)
rm(ridgePred)
set.seed(215)
library(lars)
# compute MSEs for a range of coefficient penalties as a fraction of
# final L1 norm on the interval [0,1] using cross validation
cv.res <- cv.lars(as.matrix(trainX),trainY,type="lasso",mode="fraction",plot=FALSE)
# Choose optimal value one standarf deviation away from minimum MSE
opt.frac <- min(cv.res$cv) + sd(cv.res$cv)
opt.frac <-cv.res$index[which(cv.res$cv < opt.frac)[1]]
# Compute LARS fit
lars.fit <- lars(as.matrix(trainX),trainY,type=c("lasso"))
larsPred <- predict.lars(lars.fit,newx=as.matrix(testX),type="fit")
# get last model fit
last.fit <- dim(larsPred[4]$fit)[2]
predLars <- larsPred[4]$fit[,last.fit]
RMSE(predLars,observed)
result <- cbind(result,predLars)
lars_residual <- predLars - observed
residuals <- cbind(residuals, lars_residual)
rm(lars_residual)
rm(predLars)
rm(larsPred)
set.seed(215)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(trainX, trainY, method = "earth",tuneGrid = marsGrid,trControl = trainControl(method = "cv"))
predMars <- predict(marsTuned,testX)
RMSE(predMars,observed)
result <- cbind(result,predMars)
mars_residual <- predMars - observed
residuals <- cbind(residuals, mars_residual)
rm(mars_residual)
rm(predMars)
data(longley)
# fit model
fit <- gbm(Employed~., data=longley, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
library(gbm)
# load data
data(longley)
# fit model
fit <- gbm(Employed~., data=longley, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
library(gbm)
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predGBoost <- predict(fit,testX )
# summarize accuracy
rmse <- mean(( observed - predGBoost)^2)
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predGBoost <- predict(fit,trainData)
# summarize accuracy
rmse <- mean(( observed - predGBoost)^2)
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
predGBoost <- predict(fit,df[-TrainRow, ])
predGBoost <- predict(fit,df[-TrainRow, ],n.trees=gbmFit1$bestTune$.n.trees)
predGBoost <- predict(fit,df[-TrainRow, ],n.trees=fit$bestTune$.n.trees)
fit
fit$bestTune
fit$bestTune$n.trees
predGBoost <- predict(fit,df[-TrainRow, ],n.trees=5)
?gbm
summary(fit)
rmse <- mean(( observed - predGBoost)^2)
rmse
quit()
