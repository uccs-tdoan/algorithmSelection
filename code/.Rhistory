c(1.1, 9, 3.14)
z <- c(1.1, 9, 3.14)
?c
z
c(z,555,z)
z * 2 + 100
my_sqrt <- swrt(z-1)
my_sqrt <- sqrt(z-1)
my_sqrt
my_div z/my_sqrt
my_div <- z/my_sqrt
my+div
my_div
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10)
z * 2 + 1000
my_sqrt
my_div
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
x[c(3,5,7)]
x[0]
x[3000]
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)$sigma
summary(fit)
data(mtcars)
str(mtcars)
fit <- lm(mpg~wt,data=mtcars)
sumCoef <- summary(fit)$coefficients
sumCoef[2,1] + c(-1,1)*qt(0.975,df=fit$df) * sumCoef[2,2]
sumCoef[1,1] + c(-1,1)*qt(0.975,df=fit$df) * sumCoef[1,2]
help(mtcars)
library(Hmisc)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
colnames(concrete)
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
index
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
From this plot we should probably cut the outcome in 4 categories
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
featurePlot(x = training[, names], y = cutCS, plot = "box")
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
A1
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
C2
A2 <- C2$overall[1]
A2
install.packages("IPSUR", dependencies = TRUE)
library(IPSUR)
read(IPSUR)
quit()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
library(Hmisc)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +  theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +  theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients[3]
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
compare <- anova(fit1, fit2)
compare$Pr
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the hat diagonal for the most influential point
fit <- lm(y ~ x)
hatvalues(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the slope dfbeta for the point with the highest hat value.
fit <- lm(y ~ x)
dfbetas(fit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
library(rattle)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
set.seed(125)
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
install.packages("pgmm")
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
model <- train(Area ~ ., data = olive, method = "rpart2")
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata = newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(trainSA$chd, predict(model, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(rattle)
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
library(rpart)
library(ggplot2)
library(rattle)
training<-segmentationOriginal[segmentationOriginal$Case=="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
model<-train(Class ~ .,data = training, method = "rpart")
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
inTrain
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
Library(caret)
library(caret)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(randomForest)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
imps
library(randomForest)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit) # as.factor(cyl)8  -6.0709
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr # 0.1037502
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5] # 0.9945734
hatvalues(fit5)
dfbetas(fit5)[5, 2] # -133.8226
library(dplyr)
install.packages("hflights")
glimpse(hflights)
library(hflights)
glimpse(hflights)
quit()
library(MASS)
quit()
quit()
load(diamond)
load(diamonds)
library(ggthemes)
install.packages('knitr', dependencies = T)
library(knitr)
library(devtools)
install_version("colorspace","1.2-4")
library(ggplot2)
load(diamond)
load(diamonds)
head(diamonds)
dim(diamonds)
str(diamonds)
diamonds$color
?diamond
?diamonds
ggplot(data=diamonds,aes(y=price)) +geom_hist()
ggplot(data=diamonds,aes(y=price)) +geom_histogram()
ggplot(data=diamonds,aes(y=price)) +geom_histogram(stat="bin")
names(diamonds)
qplot(x=price,data=diamonds)
library(ggthemes)
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete(breaks =1:31)
ggplot(aes(x=price),data=diamonds) + geom_histogram()
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete()
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete(breaks =c(0,5000.10000,15000,20000)
)
summary(diamonds)
summary(diamond[diamonds$price <500])
summary(diamonds[diamonds$price <500])
diamonds[diamonds$price <500]
diamonds[diamonds$price <500,]
summary(diamonds[diamonds$price <500,])
dim(diamonds[diamonds$price <500,])
count(diamonds[diamonds$price <500,])
size(diamonds[diamonds$price <500,])
dim(diamonds[diamonds$price <250,])
load(diamonds)
data(diamonds)
dim(diamonds[diamonds$price <250,])
dim(diamonds[diamonds$price > 15000,])
dim(diamonds[diamonds$price >= 15000,])
library("lubridate")
quit()
install.packages("RandomFields")
install.packages(c("arm", "BH", "boot", "caret", "chron", "clue", "cluster", "CORElearn", "devtools", "distrom", "dplyr", "earth", "ergm", "ergm.count", "expsmooth", "FactoMineR", "ForeCA", "forecast", "formatR", "Formula", "gam", "gamlr", "gdata", "geosphere", "ggplot2", "ggvis", "glmnet", "gnm", "gplots", "gtools", "h2o", "HH", "highr", "Hmisc", "jsonlite", "latentnet", "lmtest", "manipulate", "markdown", "MASS", "mda", "mgcv", "mime", "mixtools", "network", "NLP", "numDeriv", "openNLP", "party", "partykit", "pgmm", "plotmo", "plotrix", "plyr", "pROC", "qdap", "qdapDictionaries", "qdapRegex", "Quandl", "R.utils", "Rcpp", "RcppArmadillo", "RCurl", "rgl", "rmarkdown", "robustbase", "ROCR", "roxygen2", "RSNNS", "rstudioapi", "sandwich", "scales", "shiny", "sp", "statmod", "stringr", "strucchange", "survival", "tergm", "testthat", "textcat", "textir", "tm", "vcd", "VGAM", "XML", "yhatr", "zoo"))
install.packages(c("boot", "cluster", "codetools", "lattice", "MASS", "Matrix", "mgcv", "survival"), lib="C:/Program Files/R/R-3.1.3/library")
setwd("C:/algorithmSelection/code")
df <- read.csv("../Alldata.csv")
df <- read.csv("../data/Alldata.csv")
str(df)
df <- df[-c(1:7,10,12:14)]
str(df)
dim(df)
library(caret)
library(e1071)
library(ggplot2)
library(MASS)
simpleMod <- dummyVars(~., data=df,levelsOnly= TRUE)
df <- predict(simpleMod, df)
df <- as.data.frame(df)
write.table(df,file="Train.csv",sep=",",row.names=FALSE,col.names=TRUE)
set.seed(215)
TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= FALSE)
trainData <- df[TrainRow,]
ctrl <- trainControl(method = "repeatedcv", repeats = 5,number=10)
trainX <- df[TrainRow,1:ncol(df)-1]
trainY<-df[TrainRow,ncol(df)]
testX <- df[-TrainRow, 1:ncol(df)-1]
observed <- df[-TrainRow,ncol(df)]
m5Tune <- train(trainX, trainY,
method = "M5",
trControl = trainControl(method = "cv"),
control = Weka_control(M = 10))
plot(m5Tune)
m5tree <- M5P(trainY ~ ., data = trainX,  control = Weka_control(M = 10))
set.seed(215)
nnetAvg <- avNNet(trainX, trainY,  size = 5,  decay = 0.01,  repeats = 5,
linout = TRUE, trace = FALSE,  maxit = 500,   allowParallel= TRUE)
predNN <- predict(nnetAvg, testX)
RMSE(predNN,observed)
result <- cbind(observed,predNN)
NN_residual <- predNN - observed
set.seed(215)
runPCR <- train(x = trainX, y = trainY, method = "pcr", trControl = ctrl,tuneLenght=25)
predPCR <- predict(runPCR, newdata = testX)
RMSE(predPCR,observed)
result <- cbind(result,predPCR)
pcr_residual = predPCR - observed
rm(predPCR)
set.seed(215)
runPLS <- train(x = trainX, y = trainY, method = "pls", preProcess=c("center","scale"),trControl = ctrl,tuneLenght=25)
predPLS <- predict(runPLS, newdata = testX)
RMSE(predPLS,observed)
result <- cbind(result,predPLS)
PLS_residual <- predPLS - observed
residuals <- cbind(pcr_residual,PLS_residual)
rm(predPLS)
rm(pcr_residual)
rm(PLS_residual)
PLS_residual <- predPLS - observed
residuals <- cbind(pcr_residual,PLS_residual)
rm(predPLS)
rm(pcr_residual)
rm(PLS_residual)
set.seed(215)
ridgeGrid <- data.frame(.lambda = seq( .1,4, length = 20))
ridgeRegFit <- train(trainX,trainY, method ="ridge",tuneGrid = ridgeGrid, trControl=ctrl, preProc=c("center","scale"))
ridgeRegFit
ridgeModel <- enet(x = as.matrix(trainX), y = trainY,lambda=0.1)
ridgePred <- predict(ridgeModel,newx=as.matrix(testX),s=1,mode="fraction",type="fit")
RMSE(ridgePred$fit,observed)
predRidge <- ridgePred$fit
result <- cbind(result,predRidge)
ridge_residual <- predRidge - observed
residuals <- cbind(residuals,ridge_residual)
rm(ridge_residual)
rm(predRidge)
rm(ridgePred)
set.seed(215)
library(lars)
lars.fit <- lars(as.matrix(trainX),trainY,type=c("lasso"))
larsPred <- predict.lars(lars.fit,newx=as.matrix(testX),type="fit")
# get last model fit
last.fit <- dim(larsPred[4]$fit)[2]
predLars <- larsPred[4]$fit[,last.fit]
RMSE(predLars,observed)
result <- cbind(result,predLars)
lars_residual <- predLars - observed
residuals <- cbind(residuals, lars_residual)
rm(lars_residual)
rm(predLars)
rm(larsPred)
set.seed(215)
library(kernlab)
svmRModel <- train(x = trainX,trainY,method = "svmRadial",preProc = c("center", "scale"), tuneLength = 10,trControl = trainControl(method = "cv"))
#Tuning parameter 'sigma' was held constant at a value of 0.0108882
#RMSE was used to select the optimal model using  the smallest value.
#The final values used for the model were sigma = 0.0108882 and C = 16.
svmRModel$finalModel
#SV type: eps-svr  (regression)
#parameter : epsilon = 0.1  cost C = 8
#svmFit <- ksvm(x = trainX, y = trainY,kernel ="rbfdot", kpar = "automatic",C = 8, epsilon = 0.1)
predSVMR <- predict(svmRModel, newdata = testX)
RMSE(predSVMR,observed)  # 1807.155 after Box Cox, RMSE : 1.393408
result <- cbind(result,predSVMR)
SVMR_residual <- predSVMR - observed
residuals <- cbind(residuals, SVMR_residual)
rm(SVMR_residual)
rm(predSVMR)
set.seed(215)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(trainX, trainY, method = "earth",tuneGrid = marsGrid,trControl = trainControl(method = "cv"))
predMars <- predict(marsTuned,testX)
RMSE(predMars,observed)
result <- cbind(result,predMars)
mars_residual <- predMars - observed
residuals <- cbind(residuals, mars_residual)
rm(mars_residual)
rm(predMars)
set.seed(215)
knnTuned <- train(trainX, trainY,method = "knn",preProc = c("center", "scale"),
tuneGrid = data.frame(.k = 1:20),trControl = trainControl(method = "cv"))
predKNN <- predict(knnTuned,testX)
RMSE(predKNN, observed)
result <- cbind(result,predKNN)
KNNR_residual <- predKNN - observed
residuals <- cbind(residuals, KNNR_residual)
rm(KNNR_residual)
rm(predKNN)
# alternatively, postResample(pred = knnPred, obs = testY)
write.table(result,file="preditedSAR.csv",sep=",",row.names=FALSE,col.names=TRUE)
write.table(residuals,file="residuals_Compare.csv",sep=",",row.names=FALSE,col.names=TRUE)
