---
title: "Regression Models"
output: pdf_document
author: Tri Doan
Last update: June 25, 2015
---

The following codes demonstrate algorithm selection  project.

```{r,echo=FALSE}
library(caret)
library(e1071)
library(ggplot2)
library(MASS)

#setwd("C:/algoSelecMeta/finalData")
setwd("C:/algorithmSelection/code")
```

Since dataset contains all features that might be not all useful. We eliminate first 6 attributes as well as 25th feature (Dataset feature).



```{r, echo=FALSE}
df <- read.csv("../data/Alldata.csv")

df <- df[-c(1:7,10,12:14)]
```
Next, we convert category feature (algorithm attribute) into binary feature.
```{r}
simpleMod <- dummyVars(~., data=df,levelsOnly= TRUE)
df <- predict(simpleMod, df)
df <- as.data.frame(df)

write.table(df,file="Train.csv",sep=",",row.names=FALSE,col.names=TRUE)    
```
 We can use Trains.csv or continue for remaining code
 
```{r, echo=FALSE}
 set.seed(215)
 TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= FALSE)

 trainData <- df[TrainRow,]
 ctrl <- trainControl(method = "repeatedcv", repeats = 5,number=10)

 trainX <- df[TrainRow,1:ncol(df)-1]
 trainY<-df[TrainRow,ncol(df)]
 testX <- df[-TrainRow, 1:ncol(df)-1]
 observed <- df[-TrainRow,ncol(df)]


```
We use different regression techniques such as
Non-linear regression with decision Tree includes 
1. Classification and Regression Trees
2. Conditional Decision Trees
3. Model Trees
4. Rule System
5. Bagging CART
6. Random Forest
7. Gradient Boosted Machine
8. Cubist

Other non-linear regression includes
1. Neuron Network
2. Principle Components Regression
3. generate Support Vector Regression (SVR)
4. generate KNN regression
5. Partial Least Square (PLS)
6. Generate Elastic Net model
7. generate Ridge Regression
8. generate LARS model
9. Generate Multivariate Adaptive Regression Splines  (MARS)

loading libraries
```{R }
library(rpart)
library(party)
library(RWeka)
library(ipred)
library(randomForest)
library(gbm)
library(Cubist)

```
1. Classification and Regression Trees
  rmse = 0.9290579
  
```{r}
fit <- rpart(SAR~., data=trainData, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predCART <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCART)^2)
result <- cbind(observed,predCART)
CART_residual <- predCART - observed

```
 2. Conditional Decision Trees
 RMSE = 0.9165631
```{r}
# fit model
fit <- ctree(SAR~., data=trainData, controls=ctree_control(minsplit=2,minbucket=2,testtype="Univariate"))
# summarize the fit
summary(fit)
# make predictions
predCTree <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCTree)^2)
result <- cbind(result,predCTree)
CTree_residual <- predCTree - observed

```
3. Model Tree (M5P in weka)

We tune M5P and using MP5 tree to calculate
```{r}
 m5Tune <- train(trainX, trainY, 
                 method = "M5", 
                 trControl = trainControl(method = "cv"), 
                 control = Weka_control(M = 10))

plot(m5Tune)
m5tree <- M5P(trainY ~ ., data = trainX,  control = Weka_control(M = 10))

# Alternative, directly us  RWeka package
# RMSE = 0.9308284

fit <- M5P(SAR~., data=trainData)
# summarize the fit
summary(fit)
# make predictions
predM5P <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predM5P)^2)
result <- cbind(result,predM5P)
M5P_residual <- predM5P - observed

```
4. Rule System
RMSE= 0.9165631

```{r}

# fit model
fit <- M5Rules(SAR~., data= trainX)
# summarize the fit
summary(fit)
# make predictions
predRule <- predict(fit, testX)
# summarize accuracy
rmse <- mean(( observed - predRule)^2)
result <- cbind(result,predRule)
Rule_residual <- predRule - observed
```
5. Bagging CART
   RMSE = 0.9251395
  
```{r}  

# fit model
fit <- bagging(SAR~., data= trainData, control=rpart.control(minsplit=5))
# summarize the fit
summary(fit)
# make predictions
predBagCART <- predict(fit, testX)
# summarize accuracy
rmse <- mean(( observed- predBagCART)^2)
result <- cbind(result,predBagCART)
BagCart_residual <- predBagCART - observed

```
6. Random Forest
   RMSE = 0.9215616

```{r}
# fit model
fit <- randomForest(SAR~., data=trainData)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predictions)^2)

```
7. Gradient Boosted Machine
   RMSE =
   
```{r}   
# fit model
fit <- gbm(SAR~., data=trainData, distribution="gaussian")
# summarize the fit
summary(fit)
# make predictions
predGBoost <- predict(fit,testX )
# summarize accuracy
rmse <- mean(( observed - predGBoost)^2)
result <- cbind(result, predGBoost)
GBoost_residual <- predGBoost - observed
```
8. Cubist
 RMSE = 0.902461

```{r}
# fit model
fit <- cubist(trainX, trainY)
# summarize the fit
summary(fit)
# make predictions
predCubist <- predict(fit, testX)
# summarize accuracy
rmse <- mean((observed - predCubist)^2)

```

Non-linear regression 
1. Neuron Network 

Note: Compute confidence interval of prediction
predict(model, new = testX, inteval= "prediction")
predict(model, new = testX, inteval= "confidence")
build neuron network with 5 hidden unit
RMSE = 5.433054   0.9699801

```{r}
set.seed(215)
nnetAvg <- avNNet(trainX, trainY,  size = 5,  decay = 0.01,  repeats = 5,
        linout = TRUE, trace = FALSE,  maxit = 500,   allowParallel= TRUE)
predNN <- predict(nnetAvg, testX)
RMSE(predNN,observed)
result <- cbind(observed,predNN)
NN_residual <- predNN - observed
```
2.  Built Principle Components Regression model
 RMSE=  0.9713516
 
```{r,echo=FALSE}
set.seed(215)
runPCR <- train(x = trainX, y = trainY, method = "pcr", trControl = ctrl,tuneLenght=25)
predPCR <- predict(runPCR, newdata = testX)

RMSE(predPCR,observed)

result <- cbind(result,predPCR)
pcr_residual = predPCR - observed

rm(predPCR)

```
3. generate Support Vector Regression (SVR) model
  RMSE=   0.9640276
  
```{r, echo=FALSE}
set.seed(215)  
library(kernlab)
svmRModel <- train(x = trainX,trainY,method = "svmRadial",preProc = c("center", "scale"), tuneLength = 10,trControl = trainControl(method = "cv")) 
#Tuning parameter 'sigma' was held constant at a value of 0.0108882
#RMSE was used to select the optimal model using  the smallest value.
#The final values used for the model were sigma = 0.0108882 and C = 16.
svmRModel$finalModel
#SV type: eps-svr  (regression) 
#parameter : epsilon = 0.1  cost C = 8
#svmFit <- ksvm(x = trainX, y = trainY,kernel ="rbfdot", kpar = "automatic",C = 8, epsilon = 0.1)


predSVMR <- predict(svmRModel, newdata = testX)
RMSE(predSVMR,observed)  # 1807.155 after Box Cox, RMSE : 1.393408 
result <- cbind(result,predSVMR)
SVMR_residual <- predSVMR - observed
residuals <- cbind(residuals, SVMR_residual)

rm(SVMR_residual)
rm(predSVMR)

```
4.  generate KNN regression
 RMSE = 0.9691691
  
```{r , echo = FALSE}
set.seed(215)
knnTuned <- train(trainX, trainY,method = "knn",preProc = c("center", "scale"),
                  tuneGrid = data.frame(.k = 1:20),trControl = trainControl(method = "cv"))
predKNN <- predict(knnTuned,testX)
RMSE(predKNN, observed)
result <- cbind(result,predKNN)

KNNR_residual <- predKNN - observed
residuals <- cbind(residuals, KNNR_residual)

rm(KNNR_residual)
rm(predKNN)

```
5. Generate Partial Least Square (PLS) model
 RMSE = 0.9669828

```{r,echo=FALSE}
set.seed(215)
runPLS <- train(x = trainX, y = trainY, method = "pls", preProcess=c("center","scale"),trControl = ctrl,tuneLenght=25)
predPLS <- predict(runPLS, newdata = testX)

RMSE(predPLS,observed)
result <- cbind(result,predPLS)


PLS_residual <- predPLS - observed
residuals <- cbind(pcr_residual,PLS_residual)
rm(predPLS)
#rm(pcr_residual)
rm(PLS_residual)

```
6. Generate Elastic Net model
 RMSE  =  0.9668436    

```{r, echo=FALSE}
set.seed(215)  
library(elasticnet)
enetGrid <- expand.grid(lambda = c(0, .001, .01, .1, 1), fraction = seq(.05, 1, length = 20))
runENet <-  train(x = trainX, y = trainY, method = "enet", preProcess=c("center","scale"),trControl = ctrl,
                  tuneGrid=enetGrid) 
enetModel <- enet(x = as.matrix(trainX), y = trainY, lambda = 0, normalize = TRUE)
predEnet <- predict(enetModel, newx = as.matrix(testX), s = .1, mode = "fraction",    type = "fit")

RMSE(predEnet$fit,observed)
Elastic <- predEnet$fit

result <- cbind(result,Elastic)

Elastic_residual <- predEnet$fit - observed
residuals <- cbind(residuals,Elastic_residual)
rm(Elastic_residual)

rm(predEnet)  
rm(Elastic)
```
 7. generate Ridge Regression
 RMSE = 0.9669102

```{r, echo=FALSE}
set.seed(215)
ridgeGrid <- data.frame(.lambda = seq( .1,4, length = 20))
ridgeRegFit <- train(trainX,trainY, method ="ridge",tuneGrid = ridgeGrid, trControl=ctrl, preProc=c("center","scale"))
ridgeRegFit

ridgeModel <- enet(x = as.matrix(trainX), y = trainY,lambda=0.1)
ridgePred <- predict(ridgeModel,newx=as.matrix(testX),s=1,mode="fraction",type="fit")


RMSE(ridgePred$fit,observed)
predRidge <- ridgePred$fit 
result <- cbind(result,predRidge)

ridge_residual <- predRidge - observed
residuals <- cbind(residuals,ridge_residual)
rm(ridge_residual)


rm(predRidge)
rm(ridgePred)  

```
 8. generate LARS model
 RMSE=   0.9668436
  
 eg: lassoFit <- lars(as.matrix(trainX),trainY,type=c("lasso"))
 lassoPred <- predict.lars(lassoFit,newx=as.matrix(testX),type="fit")

 RMSE(lassoPred$fit,observed)
 predLasso <- lassoPred$fit 
  
```{r, echo=FALSE}
set.seed(215)
library(lars)
# compute MSEs for a range of coefficient penalties as a fraction of 
# final L1 norm on the interval [0,1] using cross validation
cv.res <- cv.lars(as.matrix(trainX),trainY,type="lasso",mode="fraction",plot=FALSE)
# Choose optimal value one standarf deviation away from minimum MSE
opt.frac <- min(cv.res$cv) + sd(cv.res$cv)
opt.frac <-cv.res$index[which(cv.res$cv < opt.frac)[1]]
# Compute LARS fit
lars.fit <- lars(as.matrix(trainX),trainY,type=c("lasso"))
larsPred <- predict.lars(lars.fit,newx=as.matrix(testX),type="fit")
# get last model fit
last.fit <- dim(larsPred[4]$fit)[2]

predLars <- larsPred[4]$fit[,last.fit]

RMSE(predLars,observed)

result <- cbind(result,predLars)

lars_residual <- predLars - observed
residuals <- cbind(residuals, lars_residual)
rm(lars_residual)


rm(predLars)
rm(larsPred)  

```
 9. Generate MARS
 RMSE =  0.9626668

```{r, echo= FALSE}
set.seed(215)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(trainX, trainY, method = "earth",tuneGrid = marsGrid,trControl = trainControl(method = "cv"))
predMars <- predict(marsTuned,testX)
RMSE(predMars,observed)
result <- cbind(result,predMars)
mars_residual <- predMars - observed
residuals <- cbind(residuals, mars_residual)

rm(mars_residual)
rm(predMars) 

# alternatively, postResample(pred = knnPred, obs = testY)
write.table(result,file="preditedSAR.csv",sep=",",row.names=FALSE,col.names=TRUE)    
write.table(residuals,file="residuals_Compare.csv",sep=",",row.names=FALSE,col.names=TRUE)    

```
  plot residual histogram of regression models

```{r, echo=FALSE}
 df <- read.csv("residuals_compare.csv")
 dat <- melt(df)
 pdf("residualHistogram.pdf", width=6, height=5)
 p1 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'pcr_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("PCR residuals")

 p2 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'PLS_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("PLS residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p3 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'Elastic_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("Elastic Net residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p4 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'ridge_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("ridge residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p5 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'lars_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("Lars residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

 p6 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'SVMR_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("SVR residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))


p7 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'mars_residual'),fill = "black", alpha = 0.2,binwidth=10) +ggtitle("mars residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

p8 <- ggplot(dat,aes(x=value)) + geom_histogram(data=subset(dat,variable == 'KNNR_residual'),fill = "black", alpha = 0.2,binwidth=10) + ggtitle("KNN residuals") + scale_x_continuous(breaks=c(-20,-10,10,20))

p<- grid.arrange(p1,p2, p3, p4,p5,p6,p7,p8, nrow=2)
print(p)
 
dev.off()
```
 compute 95% confidence interval (CI)
```{r, echo=FALSE}
 # PCR : 2.5% = -5.735264 ; 97.5% =  9.441427 
 quantile(df$pcr_residual,.025) 
 quantile(df$pcr_residual,.975)
 
 # PLS:  2.5% = -10.52431;  97.5% = 5.875617 
 quantile(df$PLS_residual,.025) 
 quantile(df$PLS_residual,.975)
 
 # Elastic: 2.5% = -7.38707  ; 97.5% = 5.639585 
 quantile(df$Elastic_residual,.025) 
 quantile(df$Elastic_residual,.975)

 # Ridge: 2.5% = -10.46934  ; 97.5% = 5.877543 
 quantile(df$ridge_residual,.025) 
 quantile(df$ridge_residual,.975)

 # LARS: 2.5% = -5.214255 ; 97.5% = 5.171606  
 quantile(df$lars_residual,.025) 
 quantile(df$lars_residual,.975)

 # SVMR_residual : 2.5% = -4.025373 ; 97.5% = 3.676516 ; 
 quantile(df$SVMR_residual,.025) 
 quantile(df$SVMR_residual,.975)

 # mars : 2.5% = -1.281899 ; 97.5%  = 1.152502 
 quantile(df$mars_residual,.025) 
 quantile(df$mars_residual,.975)

 # KNNR: 2.5% = -6.479854 ; 97.5% = 9.357834 
 quantile(df$KNNR_residual,.025) 
 quantile(df$KNNR_residual,.975)
```
 compute MAD : Median absolute quitdeviation
```{r, echo= FALSE}
 # MAD_PCR = 1.552573
  mad_pcr = mad(df$pcr_residual,center=median(df$pcr_residual))
 
 # MAD_PLS = 2.425057
  mad_pls = mad(df$PLS_residual,center=median(df$PLS_residual))
 
 # MAD_Elastic = 2.608583
  mad_enet = mad(df$Elastic_residual,center=median(df$Elastic_residual))

 # MAD_Ridge = 2.185359
 mad_ridge = mad(df$ridge_residual,center=median(df$ridge_residual))
 
 # MAD_Lars = 1.949095
 mad_lars = mad(df$lars_residual,center=median(df$lars_residual))

 # MAD_SVR = 1.376014
 mad_svr = mad(df$SVMR_residual,center=median(df$SVMR_residual))

 # MAD_MARS =  0.3343891
  mad_mars  = mad(df$mars_residual,center=median(df$mars_residual))

 # MAD_KNN = 1.489425
  mad_knn = mad(df$KNNR_residual,center=median(df$KNNR_residual))
 
```
 compute Mean Absolute Error
```{r,echo=FALSE}
 # MAE_PCR =  2.807746
  mae_pcr = mean(abs(df$pcr_residual))
 
 # MAE_PLS = 2.379797
  mae_pls = mean(abs(df$PLS_residual))
 
 # MAE_Elastic = 2.098058
  mae_enet = mean(abs(df$Elastic_residual))

 # MAE_Ridge = 2.229552
 mae_ridge = mean(abs(df$ridge_residual))
 
 # MAE_Lars = 1.816003
 mae_lars = mean(abs(df$lars_residual))

 # MAE_SVR = 1.265414
 mae_svr = mean(abs(df$SVMR_residual))

 # MAE_MARS = 0.4152009
  mae_mars  = mean(abs(df$mars_residual))

 # MAE_KNN = 2.563048
  mae_knn = mean(abs(df$KNNR_residual))

```
Compute spearman ranking

PredPCR =  0.5416336
PredPLS =  0.516655
predElastic = 0.5853912
predRidge = 0.5537625
predSVMR = 0.7229657
predMars = 0.834728
predKNN =  0.3462306
```{r}
df <- read.csv("predictedSAR.csv")
with(df,cor(observed,predPCR,method="spearman"))
with(df,cor(observed,predPLS,method="spearman"))
with(df,cor(observed,Elastic,method="spearman"))
     
with(df,cor(observed,predRidge,method="spearman"))
with(df,cor(observed,predSVMR,method="spearman"))

with(df,cor(observed,predMars,method="spearman"))
with(df,cor(observed,predKNN,method="spearman"))
