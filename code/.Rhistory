year(this_day)
wday(this_day)
wday(this_day,label=TRUE)
this_moment <- now()
this_moment
second(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my)_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
mdy(25081985)
dmy(25081985)
ymd("192012")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 8, minutes = 34 )
this_moment
skip()
skip()
depart <-  nyc + days(2)
depart
depart <- update(depart,hours=17,minutes=34)
depart
arrive <- update(depart,hours=15,minutes=50)
arrive <- depart + hours(15) + minutes(50)
?with_tz
with_tz( "Asia/Hong_Kong")
arrive <- with_tz( "Asia/Hong_Kong")
arrive <- with_tz( arrive,"Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008",tz = "Singapore")
last_time
new_interval(arrive,last_time)
?new_interval
how_long <- new_interval(last_time,arrive,tzone=attr("Singapore"))
skip()
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch()
swirl()
install_from_swirl("Mathematical Biostatistics Boot Camp")
install_from_swirl("R Programming")
install_from_swirl(" Data Analysis")
install_from_swirl("Statistical Inference")
install.packages("neuralnet")
library(nnet)
library(swirl)
swirl()
5 + 7
x <- 5 + 7
x
y <- x-3
y
c(1.1, 9, 3.14)
z <- c(1.1, 9, 3.14)
?c
z
c(z,555,z)
z * 2 + 100
my_sqrt <- swrt(z-1)
my_sqrt <- sqrt(z-1)
my_sqrt
my_div z/my_sqrt
my_div <- z/my_sqrt
my+div
my_div
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10)
z * 2 + 1000
my_sqrt
my_div
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
x[c(3,5,7)]
x[0]
x[3000]
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)$sigma
summary(fit)
data(mtcars)
str(mtcars)
fit <- lm(mpg~wt,data=mtcars)
sumCoef <- summary(fit)$coefficients
sumCoef[2,1] + c(-1,1)*qt(0.975,df=fit$df) * sumCoef[2,2]
sumCoef[1,1] + c(-1,1)*qt(0.975,df=fit$df) * sumCoef[1,2]
help(mtcars)
library(Hmisc)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
colnames(concrete)
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
index
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
From this plot we should probably cut the outcome in 4 categories
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
featurePlot(x = training[, names], y = cutCS, plot = "box")
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
A1
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
C2
A2 <- C2$overall[1]
A2
install.packages("IPSUR", dependencies = TRUE)
library(IPSUR)
read(IPSUR)
quit()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
library(Hmisc)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +  theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +  theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients[3]
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
compare <- anova(fit1, fit2)
compare$Pr
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the hat diagonal for the most influential point
fit <- lm(y ~ x)
hatvalues(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the slope dfbeta for the point with the highest hat value.
fit <- lm(y ~ x)
dfbetas(fit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
library(rattle)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
set.seed(125)
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
install.packages("pgmm")
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
model <- train(Area ~ ., data = olive, method = "rpart2")
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata = newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(trainSA$chd, predict(model, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(rattle)
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
library(rpart)
library(ggplot2)
library(rattle)
training<-segmentationOriginal[segmentationOriginal$Case=="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
model<-train(Class ~ .,data = training, method = "rpart")
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
inTrain
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
Library(caret)
library(caret)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(randomForest)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
imps
library(randomForest)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit) # as.factor(cyl)8  -6.0709
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr # 0.1037502
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5] # 0.9945734
hatvalues(fit5)
dfbetas(fit5)[5, 2] # -133.8226
library(dplyr)
install.packages("hflights")
glimpse(hflights)
library(hflights)
glimpse(hflights)
quit()
library(MASS)
quit()
quit()
load(diamond)
load(diamonds)
library(ggthemes)
install.packages('knitr', dependencies = T)
library(knitr)
library(devtools)
install_version("colorspace","1.2-4")
library(ggplot2)
load(diamond)
load(diamonds)
head(diamonds)
dim(diamonds)
str(diamonds)
diamonds$color
?diamond
?diamonds
ggplot(data=diamonds,aes(y=price)) +geom_hist()
ggplot(data=diamonds,aes(y=price)) +geom_histogram()
ggplot(data=diamonds,aes(y=price)) +geom_histogram(stat="bin")
names(diamonds)
qplot(x=price,data=diamonds)
library(ggthemes)
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete(breaks =1:31)
ggplot(aes(x=price),data=diamonds) + geom_histogram()
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete()
ggplot(aes(x=price),data=diamonds) + geom_histogram() + scale_x_discrete(breaks =c(0,5000.10000,15000,20000)
)
summary(diamonds)
summary(diamond[diamonds$price <500])
summary(diamonds[diamonds$price <500])
diamonds[diamonds$price <500]
diamonds[diamonds$price <500,]
summary(diamonds[diamonds$price <500,])
dim(diamonds[diamonds$price <500,])
count(diamonds[diamonds$price <500,])
size(diamonds[diamonds$price <500,])
dim(diamonds[diamonds$price <250,])
load(diamonds)
data(diamonds)
dim(diamonds[diamonds$price <250,])
dim(diamonds[diamonds$price > 15000,])
dim(diamonds[diamonds$price >= 15000,])
library("lubridate")
quit()
install.packages("RandomFields")
install.packages(c("arm", "BH", "boot", "caret", "chron", "clue", "cluster", "CORElearn", "devtools", "distrom", "dplyr", "earth", "ergm", "ergm.count", "expsmooth", "FactoMineR", "ForeCA", "forecast", "formatR", "Formula", "gam", "gamlr", "gdata", "geosphere", "ggplot2", "ggvis", "glmnet", "gnm", "gplots", "gtools", "h2o", "HH", "highr", "Hmisc", "jsonlite", "latentnet", "lmtest", "manipulate", "markdown", "MASS", "mda", "mgcv", "mime", "mixtools", "network", "NLP", "numDeriv", "openNLP", "party", "partykit", "pgmm", "plotmo", "plotrix", "plyr", "pROC", "qdap", "qdapDictionaries", "qdapRegex", "Quandl", "R.utils", "Rcpp", "RcppArmadillo", "RCurl", "rgl", "rmarkdown", "robustbase", "ROCR", "roxygen2", "RSNNS", "rstudioapi", "sandwich", "scales", "shiny", "sp", "statmod", "stringr", "strucchange", "survival", "tergm", "testthat", "textcat", "textir", "tm", "vcd", "VGAM", "XML", "yhatr", "zoo"))
install.packages(c("boot", "cluster", "codetools", "lattice", "MASS", "Matrix", "mgcv", "survival"), lib="C:/Program Files/R/R-3.1.3/library")
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
install.packages("h2o", repos=(c("http://s3.amazonaws.com/h2o-release/h2o/rel-kahan/5/R", getOption("repos"))))
library(h2o)
localH2O = h2o.init()
h2o.shutdown(localH2O)
ocalH2O = h2o.init(nthreads = -1)
rm(ocal/H2O)
myVec <- c(1,2,3,4,5,6,6)
myMax <- myVec[1]
myMax
myVec[0]
n <- length(myVec[myVec==myMax])
n
library(caret)
?createDataPartition
library(e1071)
library(ggplot2)
library(MASS)
library(gbm)
setwd("C:/algorithmSelection/code")
df <- read.csv("../data/Alldata.csv")
df <- df[-c(1:7,10,12:14)]
simpleMod <- dummyVars(~., data=df,levelsOnly= TRUE)
df <- predict(simpleMod, df)
df <- as.data.frame(df)
set.seed(215)
TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= FALSE)
TrainRow
TrainRow[1:10]
set.seed(215)
TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= TRUE)
TrainRow
length(trainRow)
length(TrainRow)
size(TrainRow)
class(TrainRow)
length(TrainRow)
TrainRow[1]
TrainRow[2]
length(TrainRow[1])
length(TrainRow[[1]])
set.seed(215)
TrainRow <- createDataPartition(df[, ncol(df)], p = 0.7, list= FALSE)
TrainRow
class(TrainRow)
dim(TrainRow)
set.seed(215)
TrainRow <- createDataPartition(df[, 1], p = 0.7, list= FALSE)
TrainRow
set.seed(215)
TrainRow <- createDataPartition(ncol(df), p = 0.7, list= FALSE)
ncol(df)
nrow(df)
set.seed(215)
TrainRow <- createDataPartition(nrow(df), p = 0.7, list= FALSE)
nrow(df)
set.seed(215)
TrainRow <- createDataPartition(df[1], p = 0.7, list= FALSE)
set.seed(215)
TrainRow <- createDataPartition(df[1,], p = 0.7, list= FALSE)
set.seed(215)
TrainRow <- createDataPartition(df[nrow(df),], p = 0.7, list= FALSE)
df
